{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import iqr\n",
    "\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import Image\n",
    "import plotly.io as pio\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/egracheva/Work/PolyInfo/DFT properties/LTE_DFT.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outlier at HOMO=-10\n",
    "data = data[data['HOMO']>-10]\n",
    "# Dropping all the correlated features\n",
    "data = data.drop(['Entropy', 'ZeroPointEn', 'SumEnthal', 'SumFreeEner', 'SumEner'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:, 2:-1].values\n",
    "y = data['Linear expansion coefficient'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "x_scaled = sc_x.fit_transform(x)\n",
    "sc_y = StandardScaler()\n",
    "y_scaled = sc_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = x_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_notrain(n_units1, n_units2, n_units3, weight):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(n_units1), \n",
    "                    input_dim=int(n_components),\n",
    "                    kernel_initializer=initializers.constant(value=weight), \n",
    "                    activation='relu'))       \n",
    "    if n_units2 != 0:\n",
    "        model.add(Dense(int(n_units2), \n",
    "                        kernel_initializer=initializers.constant(value=weight), \n",
    "                        activation='relu'))\n",
    "    if n_units3 != 0:\n",
    "        model.add(Dense(int(n_units3), \n",
    "                        kernel_initializer=initializers.constant(value=weight), \n",
    "                        activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='glorot_normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss=root_mean_squared_error, optimizer='sgd')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(learning_rate, n_units1, n_units2, n_units3, weight):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(n_units1), \n",
    "                    input_dim=int(n_components), \n",
    "                    kernel_initializer='glorot_normal',\n",
    "#                     kernel_initializer=initializers.random_normal(mean=weight, stddev=0.05), \n",
    "                    activation='relu'))        \n",
    "    if n_units2 != 0:\n",
    "        model.add(Dense(int(n_units2), \n",
    "                        kernel_initializer='glorot_normal',\n",
    "#                         kernel_initializer=initializers.random_normal(mean=weight, stddev=0.05), \n",
    "                        activation='relu'))\n",
    "    if n_units3 != 0:\n",
    "        model.add(Dense(int(n_units3), \n",
    "                        kernel_initializer='glorot_normal',\n",
    "#                         kernel_initializer=initializers.random_normal(mean=weight, stddev=0.05), \n",
    "                        activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='glorot_normal'))\n",
    "    # Compile the model\n",
    "    my_adam = Adam(lr=math.pow(10,-float(learning_rate)))\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=my_adam)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(x, y, ratio = 0.8):\n",
    "    full_idx = np.array([x for x in range(x.shape[0])])\n",
    "    #np.random.seed(121)\n",
    "    train_idx = np.random.choice(full_idx, \n",
    "                                 size=math.ceil(ratio*x.shape[0]), \n",
    "                                 replace = False)\n",
    "    x_train = x[train_idx]\n",
    "    y_train = y[train_idx].reshape(-1, 1)\n",
    "    \n",
    "    valid_idx = full_idx[np.isin(full_idx, train_idx, invert=True)]\n",
    "    x_valid = x[valid_idx]\n",
    "    y_valid = y[valid_idx].reshape(-1, 1)\n",
    "    \n",
    "    return x_train, x_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_splits = 10 # 17\n",
    "n_runs = 20\n",
    "best_to_keep = 5\n",
    "rmse_threshold = 5\n",
    "min_kept = 5\n",
    "n_geoms = 3\n",
    "epochs = np.array([1, 5, 10, 15, 20])\n",
    "comment = \"ConstSearch_RandFixTrain_Median\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [float(ialpha/10.) for ialpha in range(20,40,2)]\n",
    "# batch_sizes = [4, 8, 16, 32, 64, 128]\n",
    "# n_units1_bounds = [2, 4, 8, 12, 16, 24, 32, 64]\n",
    "# n_units2_bounds = [2, 4, 8, 12, 16, 24, 32, 64]\n",
    "# n_units3_bounds = [2, 4, 8, 12, 16, 24, 32, 64]\n",
    "# weight_range = [-0.1, -0.08, -0.06, -0.04, -0.02, 0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "learning_rates = [float(ialpha/10.) for ialpha in range(20,40,2)]\n",
    "batch_sizes = [4, 8, 16, 32, 64, 128]\n",
    "n_units1_bounds = [2, 4, 8]\n",
    "n_units2_bounds = [2, 4, 8]\n",
    "n_units3_bounds = [2, 4, 8]\n",
    "weight_range = [-0.06, 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\"Runs\":n_runs,\n",
    "            \"Number of splits\":kfold_splits,\n",
    "            \"Phase 1 models kept\":best_to_keep,\n",
    "            \"RMSE threshold\":rmse_threshold,\n",
    "            \"Phase 2 minimum outcome\":min_kept,\n",
    "            \"Learning rates\":learning_rates,\n",
    "            \"First layer\":n_units1_bounds,\n",
    "            \"Second layer\":n_units2_bounds,\n",
    "            \"Third layer\":n_units3_bounds,\n",
    "            \"Comment\":comment\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/egracheva/Work/GIT/ShallowNN/Code'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory /home/egracheva/Work/GIT/ShallowNN/Outputs/Kept5_RMSEthr5_Min5_ConstSearch_RandFixTrain_Median failed\n"
     ]
    }
   ],
   "source": [
    "# detect the current working directory and print it\n",
    "path = os.getcwd()\n",
    "\n",
    "suffix = \"Kept{}_RMSEthr{}_Min{}_{}\".format(best_to_keep, rmse_threshold, min_kept, comment)\n",
    "output_dir = \"{}/Outputs/{}\".format(os.path.split(path)[0], suffix)\n",
    "\n",
    "try:  \n",
    "    os.mkdir(output_dir)\n",
    "except OSError:  \n",
    "    print (\"Creation of the directory %s failed\" % output_dir)\n",
    "else:  \n",
    "    print (\"The directory %s is created\" % output_dir)\n",
    "#os.chdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('{}/Settings.json'.format(output_dir), 'w') as json_file:  \n",
    "    json.dump(settings, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_func(x, a, b, c, d):\n",
    "#     return a*np.exp(-b*x)+c\n",
    "    return a*(x**3) + b*(x**2) + c*x + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I search for the best geometry without training the network, jsut passing the data throu it and seeing the result\n",
    "def geometry_search(x, y, n_units1_bounds, n_units2_bounds, n_units3_bounds):    \n",
    "    \n",
    "    columns = ['n_units1', 'n_units2', 'n_units3', 'mean_rmse', 'best_weight']\n",
    "    geoms = pd.DataFrame()\n",
    "    \n",
    "    for n_units1 in n_units2_bounds:\n",
    "        for n_units2 in n_units2_bounds:\n",
    "            for n_units3 in n_units3_bounds:\n",
    "                #print(\"The neural network parameters are: [{}, {}, {}]\".format(n_units1, n_units2, n_units3))\n",
    "                rmse_scores = []\n",
    "                for weight in weight_range:\n",
    "                    K.clear_session()\n",
    "                    # Create a new model\n",
    "                    model = model_notrain(n_units1, n_units2, n_units3, weight)\n",
    "                    # Evaluate without training\n",
    "                    rmse = model.evaluate(x, y, verbose=0)\n",
    "                    rmse_scores.append(rmse)\n",
    "                    #print(weight, rmse)\n",
    "                mean_rmse = np.mean(rmse)\n",
    "                best_weight = weight_range[np.argmin(rmse)]\n",
    "                geoms = geoms.append([[n_units1, n_units2, n_units3, mean_rmse, best_weight]], ignore_index=True)\n",
    "    geoms.columns = columns\n",
    "    best_geoms = geoms.sort_values(by=\"mean_rmse\").iloc[:n_geoms, :].reset_index(drop=True)\n",
    "    \n",
    "    return best_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "np.random.seed(121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Started fold #0...\n",
      "Geometry search starting...\n",
      "\n",
      "WARNING:tensorflow:From /home/egracheva/miniconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Best 3 geometries:\n",
      "   n_units1  n_units2  n_units3  mean_rmse  best_weight\n",
      "0         8         8         4   0.834412        -0.06\n",
      "1         4         8         4   0.835236        -0.06\n",
      "2         8         2         4   0.835383        -0.06\n",
      "********************************\n",
      "Architecture search starting...\n",
      "\n",
      "Current conf:\n",
      "Number of units in the 1st layer: 8.0\n",
      "Number of units in the 2nd layer: 8.0\n",
      "Number of units in the 3rd layer: 4.0\n",
      "Best weight: -0.06\n",
      "The neural network parameters are: [2.0, 4]\n",
      "The neural network parameters are: [2.0, 8]\n",
      "The neural network parameters are: [2.0, 16]\n",
      "The neural network parameters are: [2.0, 32]\n",
      "The neural network parameters are: [2.0, 64]\n",
      "The neural network parameters are: [2.0, 128]\n",
      "The neural network parameters are: [2.2, 4]\n",
      "The neural network parameters are: [2.2, 8]\n",
      "The neural network parameters are: [2.2, 16]\n",
      "The neural network parameters are: [2.2, 32]\n",
      "The neural network parameters are: [2.2, 64]\n",
      "The neural network parameters are: [2.2, 128]\n",
      "The neural network parameters are: [2.4, 4]\n",
      "The neural network parameters are: [2.4, 8]\n",
      "The neural network parameters are: [2.4, 16]\n",
      "The neural network parameters are: [2.4, 32]\n",
      "The neural network parameters are: [2.4, 64]\n",
      "The neural network parameters are: [2.4, 128]\n",
      "The neural network parameters are: [2.6, 4]\n",
      "The neural network parameters are: [2.6, 8]\n",
      "The neural network parameters are: [2.6, 16]\n",
      "The neural network parameters are: [2.6, 32]\n",
      "The neural network parameters are: [2.6, 64]\n",
      "The neural network parameters are: [2.6, 128]\n",
      "The neural network parameters are: [2.8, 4]\n",
      "The neural network parameters are: [2.8, 8]\n",
      "The neural network parameters are: [2.8, 16]\n",
      "The neural network parameters are: [2.8, 32]\n",
      "The neural network parameters are: [2.8, 64]\n",
      "The neural network parameters are: [2.8, 128]\n",
      "The neural network parameters are: [3.0, 4]\n",
      "The neural network parameters are: [3.0, 8]\n",
      "The neural network parameters are: [3.0, 16]\n",
      "The neural network parameters are: [3.0, 32]\n",
      "The neural network parameters are: [3.0, 64]\n",
      "The neural network parameters are: [3.0, 128]\n",
      "The neural network parameters are: [3.2, 4]\n",
      "The neural network parameters are: [3.2, 8]\n",
      "The neural network parameters are: [3.2, 16]\n",
      "The neural network parameters are: [3.2, 32]\n",
      "The neural network parameters are: [3.2, 64]\n",
      "The neural network parameters are: [3.2, 128]\n",
      "The neural network parameters are: [3.4, 4]\n",
      "The neural network parameters are: [3.4, 8]\n",
      "The neural network parameters are: [3.4, 16]\n",
      "The neural network parameters are: [3.4, 32]\n",
      "The neural network parameters are: [3.4, 64]\n",
      "The neural network parameters are: [3.4, 128]\n",
      "The neural network parameters are: [3.6, 4]\n",
      "The neural network parameters are: [3.6, 8]\n",
      "The neural network parameters are: [3.6, 16]\n",
      "The neural network parameters are: [3.6, 32]\n",
      "The neural network parameters are: [3.6, 64]\n",
      "The neural network parameters are: [3.6, 128]\n",
      "The neural network parameters are: [3.8, 4]\n",
      "The neural network parameters are: [3.8, 8]\n",
      "The neural network parameters are: [3.8, 16]\n",
      "The neural network parameters are: [3.8, 32]\n",
      "The neural network parameters are: [3.8, 64]\n",
      "The neural network parameters are: [3.8, 128]\n",
      "Current conf:\n",
      "Number of units in the 1st layer: 4.0\n",
      "Number of units in the 2nd layer: 8.0\n",
      "Number of units in the 3rd layer: 4.0\n",
      "Best weight: -0.06\n",
      "The neural network parameters are: [2.0, 4]\n",
      "The neural network parameters are: [2.0, 8]\n",
      "The neural network parameters are: [2.0, 16]\n",
      "The neural network parameters are: [2.0, 32]\n",
      "The neural network parameters are: [2.0, 64]\n",
      "The neural network parameters are: [2.0, 128]\n",
      "The neural network parameters are: [2.2, 4]\n",
      "The neural network parameters are: [2.2, 8]\n",
      "The neural network parameters are: [2.2, 16]\n",
      "The neural network parameters are: [2.2, 32]\n",
      "The neural network parameters are: [2.2, 64]\n",
      "The neural network parameters are: [2.2, 128]\n",
      "The neural network parameters are: [2.4, 4]\n",
      "The neural network parameters are: [2.4, 8]\n",
      "The neural network parameters are: [2.4, 16]\n",
      "The neural network parameters are: [2.4, 32]\n",
      "The neural network parameters are: [2.4, 64]\n",
      "The neural network parameters are: [2.4, 128]\n",
      "The neural network parameters are: [2.6, 4]\n",
      "The neural network parameters are: [2.6, 8]\n",
      "The neural network parameters are: [2.6, 16]\n",
      "The neural network parameters are: [2.6, 32]\n",
      "The neural network parameters are: [2.6, 64]\n",
      "The neural network parameters are: [2.6, 128]\n",
      "The neural network parameters are: [2.8, 4]\n",
      "The neural network parameters are: [2.8, 8]\n",
      "The neural network parameters are: [2.8, 16]\n",
      "The neural network parameters are: [2.8, 32]\n",
      "The neural network parameters are: [2.8, 64]\n",
      "The neural network parameters are: [2.8, 128]\n",
      "The neural network parameters are: [3.0, 4]\n",
      "The neural network parameters are: [3.0, 8]\n",
      "The neural network parameters are: [3.0, 16]\n",
      "The neural network parameters are: [3.0, 32]\n",
      "The neural network parameters are: [3.0, 64]\n",
      "The neural network parameters are: [3.0, 128]\n",
      "The neural network parameters are: [3.2, 4]\n",
      "The neural network parameters are: [3.2, 8]\n",
      "The neural network parameters are: [3.2, 16]\n",
      "The neural network parameters are: [3.2, 32]\n",
      "The neural network parameters are: [3.2, 64]\n",
      "The neural network parameters are: [3.2, 128]\n",
      "The neural network parameters are: [3.4, 4]\n",
      "The neural network parameters are: [3.4, 8]\n",
      "The neural network parameters are: [3.4, 16]\n",
      "The neural network parameters are: [3.4, 32]\n",
      "The neural network parameters are: [3.4, 64]\n",
      "The neural network parameters are: [3.4, 128]\n",
      "The neural network parameters are: [3.6, 4]\n",
      "The neural network parameters are: [3.6, 8]\n",
      "The neural network parameters are: [3.6, 16]\n",
      "The neural network parameters are: [3.6, 32]\n",
      "The neural network parameters are: [3.6, 64]\n",
      "The neural network parameters are: [3.6, 128]\n",
      "The neural network parameters are: [3.8, 4]\n",
      "The neural network parameters are: [3.8, 8]\n",
      "The neural network parameters are: [3.8, 16]\n",
      "The neural network parameters are: [3.8, 32]\n",
      "The neural network parameters are: [3.8, 64]\n",
      "The neural network parameters are: [3.8, 128]\n",
      "Current conf:\n",
      "Number of units in the 1st layer: 8.0\n",
      "Number of units in the 2nd layer: 2.0\n",
      "Number of units in the 3rd layer: 4.0\n",
      "Best weight: -0.06\n",
      "The neural network parameters are: [2.0, 4]\n",
      "The neural network parameters are: [2.0, 8]\n",
      "The neural network parameters are: [2.0, 16]\n",
      "The neural network parameters are: [2.0, 32]\n",
      "The neural network parameters are: [2.0, 64]\n",
      "The neural network parameters are: [2.0, 128]\n",
      "The neural network parameters are: [2.2, 4]\n",
      "The neural network parameters are: [2.2, 8]\n",
      "The neural network parameters are: [2.2, 16]\n",
      "The neural network parameters are: [2.2, 32]\n",
      "The neural network parameters are: [2.2, 64]\n",
      "The neural network parameters are: [2.2, 128]\n",
      "The neural network parameters are: [2.4, 4]\n",
      "The neural network parameters are: [2.4, 8]\n",
      "The neural network parameters are: [2.4, 16]\n",
      "The neural network parameters are: [2.4, 32]\n",
      "The neural network parameters are: [2.4, 64]\n",
      "The neural network parameters are: [2.4, 128]\n",
      "The neural network parameters are: [2.6, 4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dbf340e4fd65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                                 \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                                 \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                                                 verbose = 0)\n\u001b[0m\u001b[1;32m     64\u001b[0m                         \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                         \u001b[0mhistories_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/python3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt_step = 0\n",
    "n_images = x_scaled.shape[0]\n",
    "kfold = KFold(kfold_splits, shuffle = True)\n",
    "fold = 0\n",
    "folds = np.array(range(0, kfold_splits)) # The folds of the interest\n",
    "\n",
    "best_confs = []\n",
    "\n",
    "for train, test in kfold.split(x_scaled):\n",
    "    if fold in folds:\n",
    "        print(\"********************************\")\n",
    "        print(\"Started fold #{}...\".format(fold))\n",
    "        \n",
    "        x_train_val = x_scaled[train,:]\n",
    "        y_train_val = y_scaled[train,:]    \n",
    "        \n",
    "        # Grid search\n",
    "        best_score = 100 # Some large initial value\n",
    "        \n",
    "        print(\"Geometry search starting...\\n\")\n",
    "        best_geoms = geometry_search(x_train_val,\n",
    "                                     y_train_val,\n",
    "                                     n_units1_bounds,\n",
    "                                     n_units2_bounds,\n",
    "                                     n_units3_bounds)\n",
    "        \n",
    "        print(\"Best {} geometries:\".format(n_geoms))\n",
    "        print(best_geoms)\n",
    "        \n",
    "        print(\"********************************\")\n",
    "        print(\"Architecture search starting...\\n\")\n",
    "        \n",
    "        for _, geom in best_geoms.iterrows():\n",
    "            n_units1, n_units2, n_units3, _, weight = geom\n",
    "            print(\"Current conf:\")\n",
    "            print(\"Number of units in the 1st layer: {}\".format(n_units1))\n",
    "            print(\"Number of units in the 2nd layer: {}\".format(n_units2))\n",
    "            print(\"Number of units in the 3rd layer: {}\".format(n_units3))\n",
    "            print(\"Best weight: {}\".format(weight))\n",
    "            for learning_rate in learning_rates:\n",
    "                for batch_size in batch_sizes:                    \n",
    "                    print(\"The neural network parameters are: [{}, {}]\".\\\n",
    "                          format(learning_rate, batch_size))\n",
    "                    histories_train = []\n",
    "                    histories_valid = []\n",
    "                    for i in range(n_runs):\n",
    "                        x_train, x_valid, y_train, y_valid = random_split(x_train_val, \n",
    "                                                                          y_train_val, \n",
    "                                                                          ratio=0.8)\n",
    "                        K.clear_session()\n",
    "                        # Create a new model\n",
    "                        model_opt = model_train(learning_rate, \n",
    "                                                n_units1, \n",
    "                                                n_units2,\n",
    "                                                n_units3,\n",
    "                                                weight)\n",
    "                        # Callbacks\n",
    "                        history = model_opt.fit(x_train,\n",
    "                                                y_train,\n",
    "                                                batch_size = batch_size,\n",
    "                                                validation_data = (x_valid, y_valid),\n",
    "                                                epochs = 25,\n",
    "                                                verbose = 0)\n",
    "                        # Prediction\n",
    "                        histories_train.append(history.history['loss'])\n",
    "                        histories_valid.append(history.history['val_loss'])\n",
    "                    epochs = np.array([0, 3, 6, 9, 12, 15, 18, 21, 24])\n",
    "                    histories_train = np.array(histories_train)\n",
    "                    histories_valid = np.array(histories_valid)\n",
    "                    histories_train_epochs = histories_train[:, epochs]\n",
    "                    histories_valid_epochs = histories_valid[:, epochs]\n",
    "\n",
    "                    # Mean and sigma over runs\n",
    "                    means_train = np.median(histories_train_epochs, axis = 0)\n",
    "                    sigmas_train = iqr(histories_train_epochs, axis = 0)\n",
    "                    means_valid = np.median(histories_valid_epochs, axis = 0)\n",
    "                    sigmas_valid = iqr(histories_valid_epochs, axis = 0)\n",
    "                    \n",
    "                    # How much does the learning curve drop from the start of training\n",
    "                    drop_train = -np.sum(np.diff(means_train))\n",
    "                    drop_valid = -np.sum(np.diff(means_valid))\n",
    "\n",
    "                    # Check around which value does it happen (the lower the better)\n",
    "                    region_train = np.min(means_train)\n",
    "                    region_valid = np.min(means_valid)\n",
    "                    \n",
    "                    # Calculate the score based on these two values\n",
    "                    score_train = -np.log((1+drop_train/region_train)/region_train/np.power(np.mean(sigmas_train), 0.25))\n",
    "                    score_valid = -np.log((1+drop_valid/region_valid)/region_valid/np.power(np.mean(sigmas_valid), 0.25))\n",
    "                    score_sum = score_valid + score_train\n",
    "                    \n",
    "                    # Update the score and plot the curves for the new best architecture\n",
    "                    if score_sum < best_score:\n",
    "                        # New best score\n",
    "                        best_score = score_sum\n",
    "                        # Updating the best configuration\n",
    "                        best_conf = [n_units1, n_units2, n_units3, learning_rate, batch_size]\n",
    "                        \n",
    "                        # Plotting the curves\n",
    "                        data = []\n",
    "                        \n",
    "                        for j in range(n_runs):\n",
    "                            if j==0:\n",
    "                                showlegend = True\n",
    "                            else:\n",
    "                                showlegend = False\n",
    "                            hist_train = go.Scatter(x = np.arange(1,26),\n",
    "                                                 y = histories_train[j, :],\n",
    "                                                 mode = 'lines',\n",
    "                                                 line = dict(color = '#ff912b',\n",
    "                                                           width = 1),\n",
    "                                                 name = \"Training history\",\n",
    "                                                 showlegend = showlegend\n",
    "\n",
    "                                                )\n",
    "                            hist_valid = go.Scatter(x = np.arange(1,26),\n",
    "                                                     y = histories_valid[j, :],\n",
    "                                                     mode = 'lines',\n",
    "                                                     line = dict(color = '#d9d6ce', \n",
    "                                                               width = 1),\n",
    "                                                     name = \"Validation history\",\n",
    "                                                     showlegend = showlegend\n",
    "                                                    )\n",
    "                            data.append(hist_train)\n",
    "                            data.append(hist_valid)\n",
    "                            \n",
    "                        train = go.Scatter(x = epochs+1,\n",
    "                                           y = means_train,\n",
    "                                           mode = 'markers',\n",
    "                                           marker = dict(color = '#c76000',\n",
    "                                                         size = 5\n",
    "                                                        ),\n",
    "                                           error_y = dict(type = 'data',\n",
    "                                                          array = sigmas_train,\n",
    "                                                          visible = True,\n",
    "                                                          thickness = 1.5,\n",
    "                                                          width = 2\n",
    "                                                         ),\n",
    "                                           name = \"Average history value (train)\"\n",
    "                                          )\n",
    "\n",
    "                        valid = go.Scatter(x = epochs+1,\n",
    "                                           y = means_valid,\n",
    "                                           mode = 'markers',\n",
    "                                           marker = dict(color = '#bdbbb5',\n",
    "                                                         size = 5\n",
    "                                                        ),\n",
    "                                           error_y = dict(type = 'data',\n",
    "                                                          array = sigmas_valid,\n",
    "                                                          visible = True,\n",
    "                                                          thickness = 1.5,\n",
    "                                                          width= 2\n",
    "                                                         ),\n",
    "                                           name = \"Average history value (valid)\"\n",
    "                                          )\n",
    "                        data.append(train)\n",
    "                        data.append(valid)\n",
    "\n",
    "                        fig = go.Figure(data)\n",
    "\n",
    "                        fig.layout.update(title = dict(\n",
    "                            text = \"SCORE VAL: {0:0.2f} SCORE TRAIN: {1:0.2f}<br>SUM: {2:0.2f} \".\\\n",
    "                                              format(score_valid, score_train, score_sum)\n",
    "                                                      ),\n",
    "                                          annotations = [dict(\n",
    "                                x=1.17,\n",
    "                                y=0.6,\n",
    "                                align=\"left\",\n",
    "                                valign=\"top\",\n",
    "                                text=\"Learning rate: {0:0.1f}<br>Batch size: {1}\".\\\n",
    "                                              format(learning_rate, batch_size),\n",
    "                                showarrow=False,\n",
    "                                xref=\"paper\",\n",
    "                                yref=\"paper\",\n",
    "                                xanchor=\"center\",\n",
    "                                yanchor=\"top\"\n",
    "                            )]\n",
    "                                         )\n",
    "                        # Saving the plot\n",
    "                        if not os.path.exists(\"{0}/Fold_{1}\".format(output_dir, fold)):\n",
    "                            os.mkdir(\"{0}/Fold_{1}\".format(output_dir, fold))\n",
    "                        conf = \"{0}+{1}+{2}\".format(int(n_units1),int(n_units2),int(n_units3))\n",
    "                        pio.write_image(fig, file=\"{0}/Fold_{1}/nn{2}_lr{3}_batch{4}\".\\\n",
    "                                        format(output_dir, fold, conf, int(learning_rate*10), int(batch_size)),format=\"svg\")                        \n",
    "                        \n",
    "        print(\"Best score: \" + str(best_score))\n",
    "        print(\"Best configuration: \" + str(best_conf))\n",
    "\n",
    "        best_confs.append(best_conf)\n",
    "        pd.DataFrame(best_confs).to_csv(\"{}/Best_configurations.csv\".format(output_dir))\n",
    "        \n",
    "        del x_train_val\n",
    "        del y_train_val\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Finished the \" + str(fold) + \" fold\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        fold += 1\n",
    "    else:   \n",
    "        print(\"Skipped the \" + str(fold) + \" fold\")\n",
    "        print(\"----------------------------------------------------\")  \n",
    "        fold += 1\n",
    "        best_confs.append([0,0,0,0,0])\n",
    "        pd.DataFrame(best_confs).to_csv(\"{}/Best_configurations.csv\".format(output_dir))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative plotting with fitting the polynomial function to the means over runs, and counting the score based on the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative plots with polynomial fit\n",
    "data = []\n",
    "train = go.Scatter(x = epochs+1,\n",
    "                   y = means_train,\n",
    "                   mode = 'markers',\n",
    "                   marker = dict(color = '#c76000',\n",
    "                                 size = 5\n",
    "                                ),\n",
    "                   error_y = dict(type = 'data',\n",
    "                                  array = sigmas_train,\n",
    "                                  visible = True,\n",
    "                                  thickness = 1.5,\n",
    "                                  width = 2\n",
    "                                 ),\n",
    "                   name = \"Average history value (train)\"\n",
    "                  )\n",
    "\n",
    "valid = go.Scatter(x = epochs+1,\n",
    "                   y = means_valid,\n",
    "                   mode = 'markers',\n",
    "                   marker = dict(color = '#bdbbb5',\n",
    "                                 size = 5\n",
    "                                ),\n",
    "                   error_y = dict(type = 'data',\n",
    "                                  array = sigmas_valid,\n",
    "                                  visible = True,\n",
    "                                  thickness = 1.5,\n",
    "                                  width= 2\n",
    "                                 ),\n",
    "                   name = \"Average history value (valid)\"\n",
    "                  )\n",
    "data.append(train)\n",
    "data.append(valid)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    if i==0:\n",
    "        showlegend = True\n",
    "    else:\n",
    "        showlegend = False\n",
    "    hist_train = go.Scatter(x = np.arange(1,26),\n",
    "                         y = histories_train[i, :],\n",
    "                         mode = 'lines',\n",
    "                         line = dict(color = '#ff912b',\n",
    "                                   width = 1),\n",
    "                         name = \"Training history\",\n",
    "                         showlegend = showlegend\n",
    "\n",
    "                        )\n",
    "    hist_valid = go.Scatter(x = np.arange(1,26),\n",
    "                             y = histories_valid[i, :],\n",
    "                             mode = 'lines',\n",
    "                             line = dict(color = '#d9d6ce', \n",
    "                                       width = 1),\n",
    "                             name = \"Validation history\",\n",
    "                             showlegend = showlegend\n",
    "                            )\n",
    "    data.append(hist_train)\n",
    "    data.append(hist_valid)\n",
    "# Sometimes the fitter fails to fit the function to the data\n",
    "try:\n",
    "    popt_train, _ = curve_fit(polynomial_func, epochs+1, means_train)\n",
    "    popt_valid, _ = curve_fit(polynomial_func, epochs+1, means_valid)\n",
    "\n",
    "    xx = np.linspace(1, 25)\n",
    "    exp_train = polynomial_func(xx, *popt_train)\n",
    "    exp_valid = polynomial_func(xx, *popt_valid)\n",
    "\n",
    "    Check how much does the exponent drop (the more the better)\n",
    "    drop_train = -np.sum(np.diff(exp_train))\n",
    "    drop_valid = -np.sum(np.diff(exp_valid))\n",
    "\n",
    "    Check around which value does it happen (the lower the better)\n",
    "    region_train = np.min(means_train)\n",
    "    region_valid = np.min(means_valid)\n",
    "    Calculate the score based on these two values\n",
    "    score_train = -np.log((1+drop_train/region_train)/region_train/np.power(np.mean(sigmas_train), 0.25))\n",
    "    score_valid = -np.log((1+drop_valid/region_valid)/region_valid/np.power(np.mean(sigmas_valid), 0.25))\n",
    "    score_sum = score_valid + score_train\n",
    "        \n",
    "        \n",
    "    expon_train = go.Scatter(x = xx, \n",
    "                       y = exp_train,\n",
    "                       mode = 'lines',\n",
    "                       line = dict(color='#c76000',\n",
    "                                   width=1.5),\n",
    "                       name = \"Polynomial fit (train)\"\n",
    "                      )\n",
    "\n",
    "    expon_valid = go.Scatter(x = xx,\n",
    "                           y = exp_valid,\n",
    "                           mode = 'lines',\n",
    "                           line = dict(color='#ff912b',\n",
    "                                       width=1.5\n",
    "                                    ),\n",
    "                           name = \"Polynomial fit (valid)\"\n",
    "                          )\n",
    "\n",
    "    data.append(expon_train)\n",
    "    data.append(expon_valid)\n",
    "\n",
    "# In case the exponential failed to fit\n",
    "except:\n",
    "    print(\"Could not fit the polynomial function to the data\")\n",
    "    score_sum = np.inf\n",
    "    score_valid = np.inf\n",
    "    score_train = np.inf\n",
    "    \n",
    "fig = go.Figure(data)\n",
    "\n",
    "fig.layout.update(title = dict(\n",
    "    text = \"SCORE VAL: {0:0.2f} SCORE TRAIN: {1:0.2f}<br>SUM: {2:0.2f} \".\\\n",
    "                      format(score_valid, score_train, score_sum)\n",
    "                              ),\n",
    "                  annotations = [dict(\n",
    "        x=1.17,\n",
    "        y=0.6,\n",
    "        align=\"left\",\n",
    "        valign=\"top\",\n",
    "        text=\"Learning rate: {0:0.2f}<br>Batch size: {1}\".\\\n",
    "                      format(learning_rate, batch_size),\n",
    "        showarrow=False,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"top\"\n",
    "    )]\n",
    "                 )\n",
    "if not os.path.exists(\"{0}/Fold_{1}\".format(output_dir, fold)):\n",
    "    os.mkdir(\"{0}/Fold_{1}\".format(output_dir, fold))\n",
    "conf = \"{0}+{1}+{2}\".format(int(n_units1),int(n_units2),int(n_units3))\n",
    "pio.write_image(fig, file=\"{0}/Fold_{1}/nn{2}_lr{3}_batch{4}\".format(output_dir, fold, conf, int(learning_rate*10), int(batch_size)),format=\"svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_fold_prediction(x_train, x_valid, learning_rate, n_units1, n_units2, n_units3, batch_size):\n",
    "      \n",
    "    prediction_train_bag = np.zeros((x_train.shape[0], n_runs))\n",
    "    prediction_valid_bag = np.zeros((x_valid.shape[0], n_runs))\n",
    "    #prediction_test_bag = np.zeros((x_test.shape[0], n_runs))\n",
    "    \n",
    "    rmse_train = np.zeros((n_runs, 1))\n",
    "    rmse_valid = np.zeros((n_runs, 1))\n",
    "    score = np.zeros((n_runs, 1))\n",
    "    \n",
    "    print(\"Running for the prediction...\")\n",
    "    for i in range(n_runs):\n",
    "        if i%1==0:\n",
    "            print(str(i) + \" runs are done\")\n",
    "        \n",
    "        model_final = model_train(learning_rate, n_units1, n_units2, n_units3, weight)\n",
    "        model_final.fit(x_train, \n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        validation_data = (x_valid, y_valid), \n",
    "                        epochs=150, \n",
    "                        verbose = 0)\n",
    "        # Prediction\n",
    "        prediction_train_bag[:,i] = model_final.predict(x_train).ravel()\n",
    "        prediction_valid_bag[:,i] = model_final.predict(x_valid).ravel()\n",
    "        #prediction_test_bag[:,i] = model_final.predict(x_test).ravel()\n",
    "                \n",
    "        rmse_train[i] = np.sqrt(mean_squared_error(y_train, prediction_train_bag[:, i]))\n",
    "        rmse_valid[i] = np.sqrt(mean_squared_error(y_valid, prediction_valid_bag[:, i]))\n",
    "        score[i] = rmse_valid[i]/rmse_train[i]\n",
    "        \n",
    "        # Cleaning up\n",
    "        ! rm -rf /tmp/weights_int.hdf5\n",
    "        K.clear_session()\n",
    "        del model_final\n",
    "    \n",
    "    # The outputs are all scaled (need to be unscaled)\n",
    "    return prediction_train_bag, prediction_val_bag, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_step = 0\n",
    "n_images = X_data_scaled_pca.shape[0]\n",
    "kfold = KFold(kfold_splits, shuffle = False)\n",
    "fold = 0\n",
    "folds = np.array(range(0,kfold_splits)) # The folds of the interest\n",
    "\n",
    "for train_valid, test in kfold.split(X_data_scaled_pca):\n",
    "    if fold in folds:\n",
    "        print(\"Started the \" + str(fold) + \" fold...\")\n",
    "        \n",
    "        ceiling = int(0.9*len(train_val))\n",
    "        train = train_valid[:ceiling]\n",
    "        valid = train_valid[ceiling:]\n",
    "        \n",
    "        x_train, x_valid = x_scaled[train,:], x_scaled[valid,:]\n",
    "        y_train, y_valid = y_scaled[train,:], y_scaled[valid,:]\n",
    "        \n",
    "        n_units1, n_units2, n_units3, learning_rate, batch_size = best_confs[fold]\n",
    "        prediction_train_bag, prediction_val_bag, score = \\\n",
    "                              each_fold_prediction(x_train, x_valid, learning_rate, n_units1, n_units2, n_units3, batch_size)\n",
    "\n",
    "        train_predictions = prediction_train_bag[:, np.array(score<rmse_threshold).ravel()]\n",
    "        valid_predictions = prediction_val_bag[:, np.array(score<rmse_threshold).ravel()]\n",
    "        #test_predictions = prediction_test_bag[:, np.array(score<rmse_threshold).ravel()]\n",
    "\n",
    "        #pd.DataFrame(test_predictions).to_csv(\"{}/Test_predictions_fold{}.csv\".format(output_dir, fold), index = False, header = False)\n",
    "        pd.DataFrame(valid_predictions).to_csv(\"{}/Validation_predictions_fold{}.csv\".format(output_dir, fold), index = False, header = False)       \n",
    "        pd.DataFrame(train_predictions).to_csv(\"{}/Train_predictions_fold{}.csv\".format(output_dir, fold), index = False, header = False)\n",
    "        \n",
    "        mean_train_predictions = np.mean(train_predictions, axis=1)\n",
    "        mean_valid_predictions = np.mean(valid_predictions, axis=1)\n",
    "        sigma_train_predictions = np.std(train_predictions, axis=1)\n",
    "        sigma_valid_predictions = np.std(valid_predictions, axis=1)\n",
    "        \n",
    "        rmse_train = mean_squared_error(y_train, mean_train_predictions)\n",
    "        rmse_valid = mean_squared_error(y_valid, mean_valid_predictions)\n",
    "        r2_train = r2_score(y_train, mean_train_predictions)\n",
    "        r2_valid = r2_score(y_valid, mean_valid_predictions)\n",
    "        mae_train = mean_absolute_error(y_train, mean_train_predictions)\n",
    "        mae_valid = mean_absolute_error(y_valid, mean_valid_predictions)\n",
    "        \n",
    "        print(\"Fold #{0}:\".format(fold))\n",
    "        print(\"RMSE train, valid: {0:0.4f}, {1:0.4f}\".format(rmse_train, rmse_valid))\n",
    "        print(\"MAE train, valid: {0:0.4f}, {1:0.4f}\".format(mae_train, mae_valid))\n",
    "        print(\"R2 score train, valid: {0:0.4f}, {1:0.4f}\".format(r2_train, r2_valid))\n",
    "        \n",
    "        \n",
    "        # ~~~~~~~~~~~~~~~~~~PLOTS~~~~~~~~~~~~~~~~~~~~\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Setting plot limits\n",
    "        y_true_min = min(np.min(y_train), np.min(y_valid))#, np.min(y_test))\n",
    "        y_true_max = max(np.max(y_train), np.max(y_valid))#, np.max(y_test))\n",
    "        y_pred_min = min(np.min(mean_train_predictions), np.min(mean_valid_predictions))#, np.min(y_test_pred))\n",
    "        y_pred_max = max(np.max(mean_train_predictions), np.max(mean_valid_predictions))#, np.max(y_test_pred))\n",
    "\n",
    "        axmin = y_true_min-0.1*(y_true_max-y_true_min)\n",
    "        axmax = y_true_max+0.1*(y_true_max-y_true_min)\n",
    "        aymin = y_pred_min-0.1*(y_pred_max-y_pred_min)\n",
    "        aymax = y_pred_max+0.1*(y_pred_max-y_pred_min)\n",
    "\n",
    "        plt.xlim(min(axmin, aymin), max(axmax, aymax))\n",
    "        plt.ylim(min(axmin, aymin), max(axmax, aymax))\n",
    "        \n",
    "        plt.errorbar(y_train, \n",
    "                    mean_train_predictions,\n",
    "                    yerr = sigma_train_predictions,\n",
    "                    fmt='o',\n",
    "                    label=\"Train\",\n",
    "                    elinewidth = 1, \n",
    "                    ms=7,\n",
    "                    mfc='#7cb2cc',\n",
    "                    markeredgewidth = 0,\n",
    "                    alpha=0.7)\n",
    "        plt.errorbar(y_valid, \n",
    "                    mean_valid_predictions,\n",
    "                    yerr = sigma_valid_predictions,\n",
    "                    elinewidth = 1,\n",
    "                    fmt='o',\n",
    "                    label=\"Validation\", \n",
    "                    ms=7, \n",
    "                    mfc='#f4a582',\n",
    "                    markeredgewidth = 0,\n",
    "                    alpha=0.7)\n",
    "        # Plot X=Y line\n",
    "        plt.plot([max(plt.xlim()[0], plt.ylim()[0]), \n",
    "                  min(plt.xlim()[1], plt.ylim()[1])],\n",
    "                 [max(plt.xlim()[0], plt.ylim()[0]), \n",
    "                  min(plt.xlim()[1], plt.ylim()[1])],\n",
    "                 ':', color = '#595f69')\n",
    "        \n",
    "        plt.xlabel('Observations LTE', fontsize = 12)\n",
    "        plt.ylabel('Predictions LTE', fontsize = 12)\n",
    "        plt.legend()\n",
    "        # ~~~~~~~~~~~~~~~~~~CHANGED HERE~~~~~~~~~~~~~~~~~~~~\n",
    "        # Added fold number\n",
    "        plt.savefig(\"{}/Plot_predictions_fold{}.png\".format(output_dir, fold)', bbox_inches='tight', dpi=80)\n",
    "        plt.close()\n",
    "        \n",
    "        del x_train, x_valid\n",
    "        del y_train, y_valid\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Finished the \" + str(fold) + \" fold\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        fold += 1\n",
    "    else:   \n",
    "        print(\"Skipped the \" + str(fold) + \" fold\")\n",
    "        print(\"----------------------------------------------------\")  \n",
    "        fold += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(17):\n",
    "    print(f)\n",
    "    test_fold = pd.read_csv(\"./Test_predictions_fold{}_PCA20_2step_5min_rmseratio5_yields5.csv\".format(f), sep=\",\", header = None).values \n",
    "\n",
    "    # Getting rid of first zeros\n",
    "    #test_fold = test_fold[:,1:50]\n",
    "    \n",
    "    final_prediction_test[f*5:f*5+5] = np.mean(sc_y.inverse_transform(test_fold), axis = 1).reshape((5,1))\n",
    "    final_std_test[f*5:f*5+5] = np.std(sc_y.inverse_transform(test_fold), axis = 1).reshape((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(Y_data, final_prediction_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

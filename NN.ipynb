{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import iqr\n",
    "\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import Image\n",
    "import plotly.io as pio\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/egracheva/Work/PolyInfo/DFT properties/LTE_DFT.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outlier at HOMO=-10\n",
    "data = data[data['HOMO']>-10]\n",
    "# Dropping all the correlated features\n",
    "data = data.drop(['Entropy', 'ZeroPointEn', 'SumEnthal', 'SumFreeEner', 'SumEner'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:, 2:-1].values\n",
    "y = data['Linear expansion coefficient'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "x_scaled = sc_x.fit_transform(x)\n",
    "sc_y = StandardScaler()\n",
    "y_scaled = sc_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = x_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_notrain(n_units1, n_units2, n_units3, weight):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(n_units1), \n",
    "                    input_dim=int(n_components),\n",
    "                    kernel_initializer=initializers.constant(value=weight), \n",
    "                    activation='relu'))       \n",
    "    if n_units2 != 0:\n",
    "        model.add(Dense(int(n_units2), \n",
    "                        kernel_initializer=initializers.constant(value=weight), \n",
    "                        activation='relu'))\n",
    "    if n_units3 != 0:\n",
    "        model.add(Dense(int(n_units3), \n",
    "                        kernel_initializer=initializers.constant(value=weight), \n",
    "                        activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='glorot_normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss=root_mean_squared_error, optimizer='sgd')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(learning_rate, n_units1, n_units2, n_units3, weight):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(n_units1), \n",
    "                    input_dim=int(n_components), \n",
    "                    kernel_initializer='glorot_normal',\n",
    "#                     kernel_initializer=initializers.random_normal(mean=weight, stddev=0.05), \n",
    "                    activation='relu'))        \n",
    "    if n_units2 != 0:\n",
    "        model.add(Dense(int(n_units2), \n",
    "                        kernel_initializer='glorot_normal',\n",
    "#                         kernel_initializer=initializers.random_normal(mean=weight, stddev=0.05), \n",
    "                        activation='relu'))\n",
    "    if n_units3 != 0:\n",
    "        model.add(Dense(int(n_units3), \n",
    "                        kernel_initializer='glorot_normal',\n",
    "#                         kernel_initializer=initializers.random_normal(mean=weight, stddev=0.05), \n",
    "                        activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='glorot_normal'))\n",
    "    # Compile the model\n",
    "    my_adam = Adam(lr=math.pow(10,-float(learning_rate)))\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=my_adam)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(x, y, ratio = 0.8):\n",
    "    full_idx = np.array([x for x in range(x.shape[0])])\n",
    "    #np.random.seed(121)\n",
    "    train_idx = np.random.choice(full_idx, \n",
    "                                 size=math.ceil(ratio*x.shape[0]), \n",
    "                                 replace = False)\n",
    "    x_train = x[train_idx]\n",
    "    y_train = y[train_idx].reshape(-1, 1)\n",
    "    \n",
    "    valid_idx = full_idx[np.isin(full_idx, train_idx, invert=True)]\n",
    "    x_valid = x[valid_idx]\n",
    "    y_valid = y[valid_idx].reshape(-1, 1)\n",
    "    \n",
    "#     x_valid = x_train\n",
    "#     y_valid = y_train\n",
    "    \n",
    "    return x_train, x_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_splits = 10 # 17\n",
    "n_runs = 20\n",
    "best_to_keep = 5\n",
    "rmse_threshold = 5\n",
    "min_kept = 5\n",
    "n_geoms = 3\n",
    "epochs = np.array([1, 5, 10, 15, 20])\n",
    "comment = \"ConstSearch_RandFixTrain_Median\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [float(ialpha/10.) for ialpha in range(20,40,2)]\n",
    "batch_sizes = [4, 8, 16, 32, 64, 128]\n",
    "n_units1_bounds = [2, 4, 8, 12, 16, 24, 32, 64]\n",
    "n_units2_bounds = [2, 4, 8, 12, 16, 24, 32, 64]\n",
    "n_units3_bounds = [2, 4, 8, 12, 16, 24, 32, 64]\n",
    "weight_range = [-0.1, -0.08, -0.06, -0.04, -0.02, 0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "# learning_rates = [float(ialpha/10.) for ialpha in range(20,40,2)]\n",
    "# batch_sizes = [4, 8, 16, 32, 64, 128]\n",
    "# n_units1_bounds = [2, 4, 8]\n",
    "# n_units2_bounds = [2, 4, 8]\n",
    "# n_units3_bounds = [2, 4, 8]\n",
    "# weight_range = [-0.06, 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\"Runs\":n_runs,\n",
    "            \"Number of splits\":kfold_splits,\n",
    "            \"Phase 1 models kept\":best_to_keep,\n",
    "            \"RMSE threshold\":rmse_threshold,\n",
    "            \"Phase 2 minimum outcome\":min_kept,\n",
    "            \"Learning rates\":learning_rates,\n",
    "            \"First layer\":n_units1_bounds,\n",
    "            \"Second layer\":n_units2_bounds,\n",
    "            \"Third layer\":n_units3_bounds,\n",
    "            \"Comment\":comment\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the current working directory and print it\n",
    "path = os.getcwd()\n",
    "\n",
    "suffix = \"Kept{}_RMSEthr{}_Min{}_{}\".format(best_to_keep, rmse_threshold, min_kept, comment)\n",
    "output_dir = \"{}/Outputs/{}\".format(os.path.split(path)[0], suffix)\n",
    "\n",
    "try:  \n",
    "    os.mkdir(output_dir)\n",
    "except OSError:  \n",
    "    print (\"Creation of the directory %s failed\" % output_dir)\n",
    "else:  \n",
    "    print (\"The directory %s is created\" % output_dir)\n",
    "#os.chdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('{}/Settings.json'.format(output_dir), 'w') as json_file:  \n",
    "    json.dump(settings, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_func(x, a, b, c, d):\n",
    "#     return a*np.exp(-b*x)+c\n",
    "    return a*(x**3) + b*(x**2) + c*x + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I search for the best geometry without training the network, jsut passing the data throu it and seeing the result\n",
    "def geometry_search(x, y, n_units1_bounds, n_units2_bounds, n_units3_bounds):    \n",
    "    \n",
    "    columns = ['n_units1', 'n_units2', 'n_units3', 'mean_rmse', 'best_weight']\n",
    "    geoms = pd.DataFrame()\n",
    "    \n",
    "    for n_units1 in n_units2_bounds:\n",
    "        for n_units2 in n_units2_bounds:\n",
    "            for n_units3 in n_units3_bounds:\n",
    "                #print(\"The neural network parameters are: [{}, {}, {}]\".format(n_units1, n_units2, n_units3))\n",
    "                rmse_scores = []\n",
    "                for weight in weight_range:\n",
    "                    K.clear_session()\n",
    "                    # Create a new model\n",
    "                    model = model_notrain(n_units1, n_units2, n_units3, weight)\n",
    "                    # Evaluate without training\n",
    "                    rmse = model.evaluate(x, y, verbose=0)\n",
    "                    rmse_scores.append(rmse)\n",
    "                    #print(weight, rmse)\n",
    "                mean_rmse = np.mean(rmse)\n",
    "                best_weight = weight_range[np.argmin(rmse)]\n",
    "                geoms = geoms.append([[n_units1, n_units2, n_units3, mean_rmse, best_weight]], ignore_index=True)\n",
    "    geoms.columns = columns\n",
    "    best_geoms = geoms.sort_values(by=\"mean_rmse\").iloc[:n_geoms, :].reset_index(drop=True)\n",
    "    \n",
    "    return best_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "np.random.seed(121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_step = 0\n",
    "n_images = x_scaled.shape[0]\n",
    "kfold = KFold(kfold_splits, shuffle = True)\n",
    "fold = 0\n",
    "folds = np.array(range(0, kfold_splits)) # The folds of the interest\n",
    "\n",
    "best_confs = []\n",
    "\n",
    "for train, test in kfold.split(x_scaled):\n",
    "    if fold in folds:\n",
    "        print(\"********************************\")\n",
    "        print(\"Started fold #{}...\".format(fold))\n",
    "        \n",
    "        x_train_val = x_scaled[train,:]\n",
    "        y_train_val = y_scaled[train,:]    \n",
    "        \n",
    "        # Grid search\n",
    "        best_score = 100 # Some large initial value\n",
    "        \n",
    "        print(\"Geometry search starting...\\n\")\n",
    "        best_geoms = geometry_search(x_train_val,\n",
    "                                     y_train_val,\n",
    "                                     n_units1_bounds,\n",
    "                                     n_units2_bounds,\n",
    "                                     n_units3_bounds)\n",
    "        \n",
    "        print(\"Best {} geometries:\".format(n_geoms))\n",
    "        print(best_geoms)\n",
    "        \n",
    "        print(\"********************************\")\n",
    "        print(\"Architecture search starting...\\n\")\n",
    "        \n",
    "        for _, geom in best_geoms.iterrows():\n",
    "            n_units1, n_units2, n_units3, _, weight = geom\n",
    "            print(\"Current conf:\")\n",
    "            print(\"Number of units in the 1st layer: {}\".format(n_units1))\n",
    "            print(\"Number of units in the 2nd layer: {}\".format(n_units2))\n",
    "            print(\"Number of units in the 3rd layer: {}\".format(n_units3))\n",
    "            print(\"Best weight: {}\".format(weight))\n",
    "            for learning_rate in learning_rates:\n",
    "                for batch_size in batch_sizes:                    \n",
    "                    print(\"The neural network parameters are: [{}, {}]\".format(learning_rate, batch_size))\n",
    "                    histories_train = []\n",
    "                    histories_valid = []\n",
    "                    for i in range(n_runs):\n",
    "                        x_train, x_valid, y_train, y_valid = random_split(x_train_val, y_train_val, ratio=0.8)\n",
    "                        K.clear_session()\n",
    "                        # Create a new model\n",
    "                        model_opt = model_train(learning_rate, n_units1, n_units2, n_units3, weight)\n",
    "                        # Callbacks\n",
    "                        history = model_opt.fit(x_train,\n",
    "                                      y_train,\n",
    "                                      batch_size = batch_size,\n",
    "                                      validation_data = (x_valid, y_valid),\n",
    "                                      epochs = 25,\n",
    "                                      verbose = 0)\n",
    "                        # Prediction\n",
    "                        histories_train.append(history.history['loss'])\n",
    "                        histories_valid.append(history.history['val_loss'])\n",
    "                    epochs = np.array([0, 3, 6, 9, 12, 15, 18, 21, 24])\n",
    "                    histories_train = np.array(histories_train)\n",
    "                    histories_valid = np.array(histories_valid)\n",
    "                    histories_train_epochs = histories_train[:, epochs]\n",
    "                    histories_valid_epochs = histories_valid[:, epochs]\n",
    "\n",
    "                    # Mean and sigma over runs\n",
    "                    means_train = np.median(histories_train_epochs, axis = 0)\n",
    "                    sigmas_train = iqr(histories_train_epochs, axis = 0)\n",
    "                    means_valid = np.median(histories_valid_epochs, axis = 0)\n",
    "                    sigmas_valid = iqr(histories_valid_epochs, axis = 0)\n",
    "                    \n",
    "                    # How much does the learning curve drop from the start of training\n",
    "                    drop_train = -np.sum(np.diff(means_train))\n",
    "                    drop_valid = -np.sum(np.diff(means_valid))\n",
    "\n",
    "                    # Check around which value does it happen (the lower the better)\n",
    "                    region_train = np.min(means_train)\n",
    "                    region_valid = np.min(means_valid)\n",
    "                    \n",
    "                    # Calculate the score based on these two values\n",
    "                    score_train = -np.log((1+drop_train/region_train)/region_train/np.power(np.mean(sigmas_train), 0.25))\n",
    "                    score_valid = -np.log((1+drop_valid/region_valid)/region_valid/np.power(np.mean(sigmas_valid), 0.25))\n",
    "                    score_sum = score_valid + score_train\n",
    "                    \n",
    "                    # Update the score and plot the curves for the new best architecture\n",
    "                    if score_sum < best_score:\n",
    "                        # New best score\n",
    "                        best_score = score_sum\n",
    "                        # Updating the best configuration\n",
    "                        best_conf = [n_units1, n_units2, n_units3, learning_rate, batch_size]\n",
    "                        \n",
    "                        # Plotting the curves\n",
    "                        data = []\n",
    "                        train = go.Scatter(x = epochs+1,\n",
    "                                           y = means_train,\n",
    "                                           mode = 'markers',\n",
    "                                           marker = dict(color = '#c76000',\n",
    "                                                         size = 5\n",
    "                                                        ),\n",
    "                                           error_y = dict(type = 'data',\n",
    "                                                          array = sigmas_train,\n",
    "                                                          visible = True,\n",
    "                                                          thickness = 1.5,\n",
    "                                                          width = 2\n",
    "                                                         ),\n",
    "                                           name = \"Average history value (train)\"\n",
    "                                          )\n",
    "\n",
    "                        valid = go.Scatter(x = epochs+1,\n",
    "                                           y = means_valid,\n",
    "                                           mode = 'markers',\n",
    "                                           marker = dict(color = '#bdbbb5',\n",
    "                                                         size = 5\n",
    "                                                        ),\n",
    "                                           error_y = dict(type = 'data',\n",
    "                                                          array = sigmas_valid,\n",
    "                                                          visible = True,\n",
    "                                                          thickness = 1.5,\n",
    "                                                          width= 2\n",
    "                                                         ),\n",
    "                                           name = \"Average history value (valid)\"\n",
    "                                          )\n",
    "\n",
    "                        for j in range(n_runs):\n",
    "                            if j==0:\n",
    "                                showlegend = True\n",
    "                            else:\n",
    "                                showlegend = False\n",
    "                            hist_train = go.Scatter(x = np.arange(1,26),\n",
    "                                                 y = histories_train[i, :],\n",
    "                                                 mode = 'lines',\n",
    "                                                 line = dict(color = '#ff912b',\n",
    "                                                           width = 1),\n",
    "                                                 name = \"Training history\",\n",
    "                                                 showlegend = showlegend\n",
    "\n",
    "                                                )\n",
    "                            hist_valid = go.Scatter(x = np.arange(1,26),\n",
    "                                                     y = histories_valid[i, :],\n",
    "                                                     mode = 'lines',\n",
    "                                                     line = dict(color = '#d9d6ce', \n",
    "                                                               width = 1),\n",
    "                                                     name = \"Validation history\",\n",
    "                                                     showlegend = showlegend\n",
    "                                                    )\n",
    "                            data.append(hist_train)\n",
    "                            data.append(hist_valid)\n",
    "                        data.append(train)\n",
    "                        data.append(valid)\n",
    "\n",
    "                        fig = go.Figure(data)\n",
    "\n",
    "                        fig.layout.update(title = dict(\n",
    "                            text = \"SCORE VAL: {0:0.2f} SCORE TRAIN: {1:0.2f}<br>SUM: {2:0.2f} \".\\\n",
    "                                              format(score_valid, score_train, score_sum)\n",
    "                                                      ),\n",
    "                                          annotations = [dict(\n",
    "                                x=1.17,\n",
    "                                y=0.6,\n",
    "                                align=\"left\",\n",
    "                                valign=\"top\",\n",
    "                                text=\"Learning rate: {0:0.2f}<br>Batch size: {1}\".\\\n",
    "                                              format(learning_rate, batch_size),\n",
    "                                showarrow=False,\n",
    "                                xref=\"paper\",\n",
    "                                yref=\"paper\",\n",
    "                                xanchor=\"center\",\n",
    "                                yanchor=\"top\"\n",
    "                            )]\n",
    "                                         )\n",
    "                        # Saving the plot\n",
    "                        if not os.path.exists(\"{0}/Fold_{1}\".format(output_dir, fold)):\n",
    "                            os.mkdir(\"{0}/Fold_{1}\".format(output_dir, fold))\n",
    "                        conf = \"{0}+{1}+{2}\".format(int(n_units1),int(n_units2),int(n_units3))\n",
    "                        pio.write_image(fig, file=\"{0}/Fold_{1}/nn{2}_lr{3}_batch{4}\".format(output_dir, fold, conf, int(learning_rate*10), int(batch_size)),format=\"svg\")\n",
    "                        # ~~~~~~~~~~~~~~~ END OF PLOTTING ~~~~~~~~~~~~~~~\n",
    "                        \n",
    "                        \n",
    "        print(\"Best score: \" + str(best_score))\n",
    "        print(\"Best configuration: \" + str(best_conf))\n",
    "\n",
    "        best_confs.append(best_conf)\n",
    "        pd.DataFrame(best_confs).to_csv(\"{}/Best_configurations.csv\".format(output_dir))\n",
    "        \n",
    "        del x_train_val\n",
    "        del y_train_val\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Finished the \" + str(fold) + \" fold\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        fold += 1\n",
    "    else:   \n",
    "        print(\"Skipped the \" + str(fold) + \" fold\")\n",
    "        print(\"----------------------------------------------------\")  \n",
    "        fold += 1\n",
    "        best_confs.append([0,0,0,0,0])\n",
    "        pd.DataFrame(best_confs).to_csv(\"{}/Best_configurations.csv\".format(output_dir))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative plots with polynomial fit\n",
    "data = []\n",
    "train = go.Scatter(x = epochs+1,\n",
    "                   y = means_train,\n",
    "                   mode = 'markers',\n",
    "                   marker = dict(color = '#c76000',\n",
    "                                 size = 5\n",
    "                                ),\n",
    "                   error_y = dict(type = 'data',\n",
    "                                  array = sigmas_train,\n",
    "                                  visible = True,\n",
    "                                  thickness = 1.5,\n",
    "                                  width = 2\n",
    "                                 ),\n",
    "                   name = \"Average history value (train)\"\n",
    "                  )\n",
    "\n",
    "valid = go.Scatter(x = epochs+1,\n",
    "                   y = means_valid,\n",
    "                   mode = 'markers',\n",
    "                   marker = dict(color = '#bdbbb5',\n",
    "                                 size = 5\n",
    "                                ),\n",
    "                   error_y = dict(type = 'data',\n",
    "                                  array = sigmas_valid,\n",
    "                                  visible = True,\n",
    "                                  thickness = 1.5,\n",
    "                                  width= 2\n",
    "                                 ),\n",
    "                   name = \"Average history value (valid)\"\n",
    "                  )\n",
    "data.append(train)\n",
    "data.append(valid)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    if i==0:\n",
    "        showlegend = True\n",
    "    else:\n",
    "        showlegend = False\n",
    "    hist_train = go.Scatter(x = np.arange(1,26),\n",
    "                         y = histories_train[i, :],\n",
    "                         mode = 'lines',\n",
    "                         line = dict(color = '#ff912b',\n",
    "                                   width = 1),\n",
    "                         name = \"Training history\",\n",
    "                         showlegend = showlegend\n",
    "\n",
    "                        )\n",
    "    hist_valid = go.Scatter(x = np.arange(1,26),\n",
    "                             y = histories_valid[i, :],\n",
    "                             mode = 'lines',\n",
    "                             line = dict(color = '#d9d6ce', \n",
    "                                       width = 1),\n",
    "                             name = \"Validation history\",\n",
    "                             showlegend = showlegend\n",
    "                            )\n",
    "    data.append(hist_train)\n",
    "    data.append(hist_valid)\n",
    "# Sometimes the fitter fails to fit the function to the data\n",
    "try:\n",
    "    popt_train, _ = curve_fit(polynomial_func, epochs+1, means_train)\n",
    "    popt_valid, _ = curve_fit(polynomial_func, epochs+1, means_valid)\n",
    "\n",
    "    xx = np.linspace(1, 25)\n",
    "    exp_train = polynomial_func(xx, *popt_train)\n",
    "    exp_valid = polynomial_func(xx, *popt_valid)\n",
    "\n",
    "    Check how much does the exponent drop (the more the better)\n",
    "    drop_train = -np.sum(np.diff(exp_train))\n",
    "    drop_valid = -np.sum(np.diff(exp_valid))\n",
    "\n",
    "    Check around which value does it happen (the lower the better)\n",
    "    region_train = np.min(means_train)\n",
    "    region_valid = np.min(means_valid)\n",
    "    Calculate the score based on these two values\n",
    "    score_train = -np.log((1+drop_train/region_train)/region_train/np.power(np.mean(sigmas_train), 0.25))\n",
    "    score_valid = -np.log((1+drop_valid/region_valid)/region_valid/np.power(np.mean(sigmas_valid), 0.25))\n",
    "    score_sum = score_valid + score_train\n",
    "        \n",
    "        \n",
    "    expon_train = go.Scatter(x = xx, \n",
    "                       y = exp_train,\n",
    "                       mode = 'lines',\n",
    "                       line = dict(color='#c76000',\n",
    "                                   width=1.5),\n",
    "                       name = \"Polynomial fit (train)\"\n",
    "                      )\n",
    "\n",
    "    expon_valid = go.Scatter(x = xx,\n",
    "                           y = exp_valid,\n",
    "                           mode = 'lines',\n",
    "                           line = dict(color='#ff912b',\n",
    "                                       width=1.5\n",
    "                                    ),\n",
    "                           name = \"Polynomial fit (valid)\"\n",
    "                          )\n",
    "\n",
    "    data.append(expon_train)\n",
    "    data.append(expon_valid)\n",
    "\n",
    "# In case the exponential failed to fit\n",
    "except:\n",
    "    print(\"Could not fit the polynomial function to the data\")\n",
    "    score_sum = np.inf\n",
    "    score_valid = np.inf\n",
    "    score_train = np.inf\n",
    "    \n",
    "fig = go.Figure(data)\n",
    "\n",
    "fig.layout.update(title = dict(\n",
    "    text = \"SCORE VAL: {0:0.2f} SCORE TRAIN: {1:0.2f}<br>SUM: {2:0.2f} \".\\\n",
    "                      format(score_valid, score_train, score_sum)\n",
    "                              ),\n",
    "                  annotations = [dict(\n",
    "        x=1.17,\n",
    "        y=0.6,\n",
    "        align=\"left\",\n",
    "        valign=\"top\",\n",
    "        text=\"Learning rate: {0:0.2f}<br>Batch size: {1}\".\\\n",
    "                      format(learning_rate, batch_size),\n",
    "        showarrow=False,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"top\"\n",
    "    )]\n",
    "                 )\n",
    "if not os.path.exists(\"{0}/Fold_{1}\".format(output_dir, fold)):\n",
    "    os.mkdir(\"{0}/Fold_{1}\".format(output_dir, fold))\n",
    "conf = \"{0}+{1}+{2}\".format(int(n_units1),int(n_units2),int(n_units3))\n",
    "pio.write_image(fig, file=\"{0}/Fold_{1}/nn{2}_lr{3}_batch{4}\".format(output_dir, fold, conf, int(learning_rate*10), int(batch_size)),format=\"svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_fold_prediction(x_train, x_valid, learning_rate, n_units1, n_units2, n_units3, batch_size):\n",
    "      \n",
    "    prediction_train_bag = np.zeros((x_train.shape[0], n_runs))\n",
    "    prediction_valid_bag = np.zeros((x_valid.shape[0], n_runs))\n",
    "    #prediction_test_bag = np.zeros((x_test.shape[0], n_runs))\n",
    "    \n",
    "    rmse_train = np.zeros((n_runs, 1))\n",
    "    rmse_valid = np.zeros((n_runs, 1))\n",
    "    score = np.zeros((n_runs, 1))\n",
    "    \n",
    "    print(\"Running for the prediction...\")\n",
    "    for i in range(n_runs):\n",
    "        if i%1==0:\n",
    "            print(str(i) + \" runs are done\")\n",
    "        \n",
    "        model_final = model_train(learning_rate, n_units1, n_units2, n_units3, weight)\n",
    "        model_final.fit(x_train, \n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        validation_data = (x_valid, y_valid), \n",
    "                        epochs=150, \n",
    "                        verbose = 0)\n",
    "        # Prediction\n",
    "        prediction_train_bag[:,i] = model_final.predict(x_train).ravel()\n",
    "        prediction_valid_bag[:,i] = model_final.predict(x_valid).ravel()\n",
    "        #prediction_test_bag[:,i] = model_final.predict(x_test).ravel()\n",
    "                \n",
    "        rmse_train[i] = np.sqrt(mean_squared_error(y_train, prediction_train_bag[:, i]))\n",
    "        rmse_valid[i] = np.sqrt(mean_squared_error(y_valid, prediction_valid_bag[:, i]))\n",
    "        score[i] = rmse_valid[i]/rmse_train[i]\n",
    "        \n",
    "        # Cleaning up\n",
    "        ! rm -rf /tmp/weights_int.hdf5\n",
    "        K.clear_session()\n",
    "        del model_final\n",
    "    \n",
    "    # The outputs are all scaled (need to be unscaled)\n",
    "    return prediction_train_bag, prediction_val_bag, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_step = 0\n",
    "n_images = X_data_scaled_pca.shape[0]\n",
    "kfold = KFold(kfold_splits, shuffle = False)\n",
    "fold = 0\n",
    "folds = np.array(range(0,kfold_splits)) # The folds of the interest\n",
    "\n",
    "for train_valid, test in kfold.split(X_data_scaled_pca):\n",
    "    if fold in folds:\n",
    "        print(\"Started the \" + str(fold) + \" fold...\")\n",
    "        \n",
    "        ceiling = int(0.9*len(train_val))\n",
    "        train = train_valid[:ceiling]\n",
    "        valid = train_valid[ceiling:]\n",
    "        \n",
    "        x_train, x_valid = x_scaled[train,:], x_scaled[valid,:]\n",
    "        y_train, y_valid = y_scaled[train,:], y_scaled[valid,:]\n",
    "        \n",
    "        n_units1, n_units2, n_units3, learning_rate, batch_size = best_confs[fold]\n",
    "        prediction_train_bag, prediction_val_bag, score = \\\n",
    "                              each_fold_prediction(x_train, x_valid, learning_rate, n_units1, n_units2, n_units3, batch_size)\n",
    "\n",
    "        train_predictions = prediction_train_bag[:, np.array(score<rmse_threshold).ravel()]\n",
    "        valid_predictions = prediction_val_bag[:, np.array(score<rmse_threshold).ravel()]\n",
    "        #test_predictions = prediction_test_bag[:, np.array(score<rmse_threshold).ravel()]\n",
    "\n",
    "        #pd.DataFrame(test_predictions).to_csv(\"{}/Test_predictions_fold{}.csv\".format(output_dir, fold), index = False, header = False)\n",
    "        pd.DataFrame(valid_predictions).to_csv(\"{}/Validation_predictions_fold{}.csv\".format(output_dir, fold), index = False, header = False)       \n",
    "        pd.DataFrame(train_predictions).to_csv(\"{}/Train_predictions_fold{}.csv\".format(output_dir, fold), index = False, header = False)\n",
    "        \n",
    "        mean_train_predictions = np.mean(train_predictions, axis=1)\n",
    "        mean_valid_predictions = np.mean(valid_predictions, axis=1)\n",
    "        sigma_train_predictions = np.std(train_predictions, axis=1)\n",
    "        sigma_valid_predictions = np.std(valid_predictions, axis=1)\n",
    "        \n",
    "        rmse_train = mean_squared_error(y_train, mean_train_predictions)\n",
    "        rmse_valid = mean_squared_error(y_valid, mean_valid_predictions)\n",
    "        r2_train = r2_score(y_train, mean_train_predictions)\n",
    "        r2_valid = r2_score(y_valid, mean_valid_predictions)\n",
    "        mae_train = mean_absolute_error(y_train, mean_train_predictions)\n",
    "        mae_valid = mean_absolute_error(y_valid, mean_valid_predictions)\n",
    "        \n",
    "        print(\"Fold #{0}:\".format(fold))\n",
    "        print(\"RMSE train, valid: {0:0.4f}, {1:0.4f}\".format(rmse_train, rmse_valid))\n",
    "        print(\"MAE train, valid: {0:0.4f}, {1:0.4f}\".format(mae_train, mae_valid))\n",
    "        print(\"R2 score train, valid: {0:0.4f}, {1:0.4f}\".format(r2_train, r2_valid))\n",
    "        \n",
    "        \n",
    "        # ~~~~~~~~~~~~~~~~~~PLOTS~~~~~~~~~~~~~~~~~~~~\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Setting plot limits\n",
    "        y_true_min = min(np.min(y_train), np.min(y_valid))#, np.min(y_test))\n",
    "        y_true_max = max(np.max(y_train), np.max(y_valid))#, np.max(y_test))\n",
    "        y_pred_min = min(np.min(mean_train_predictions), np.min(mean_valid_predictions))#, np.min(y_test_pred))\n",
    "        y_pred_max = max(np.max(mean_train_predictions), np.max(mean_valid_predictions))#, np.max(y_test_pred))\n",
    "\n",
    "        axmin = y_true_min-0.1*(y_true_max-y_true_min)\n",
    "        axmax = y_true_max+0.1*(y_true_max-y_true_min)\n",
    "        aymin = y_pred_min-0.1*(y_pred_max-y_pred_min)\n",
    "        aymax = y_pred_max+0.1*(y_pred_max-y_pred_min)\n",
    "\n",
    "        plt.xlim(min(axmin, aymin), max(axmax, aymax))\n",
    "        plt.ylim(min(axmin, aymin), max(axmax, aymax))\n",
    "        \n",
    "        plt.errorbar(y_train, \n",
    "                    mean_train_predictions,\n",
    "                    yerr = sigma_train_predictions,\n",
    "                    fmt='o',\n",
    "                    label=\"Train\",\n",
    "                    elinewidth = 1, \n",
    "                    ms=7,\n",
    "                    mfc='#7cb2cc',\n",
    "                    markeredgewidth = 0,\n",
    "                    alpha=0.7)\n",
    "        plt.errorbar(y_valid, \n",
    "                    mean_valid_predictions,\n",
    "                    yerr = sigma_valid_predictions,\n",
    "                    elinewidth = 1,\n",
    "                    fmt='o',\n",
    "                    label=\"Validation\", \n",
    "                    ms=7, \n",
    "                    mfc='#f4a582',\n",
    "                    markeredgewidth = 0,\n",
    "                    alpha=0.7)\n",
    "        # Plot X=Y line\n",
    "        plt.plot([max(plt.xlim()[0], plt.ylim()[0]), \n",
    "                  min(plt.xlim()[1], plt.ylim()[1])],\n",
    "                 [max(plt.xlim()[0], plt.ylim()[0]), \n",
    "                  min(plt.xlim()[1], plt.ylim()[1])],\n",
    "                 ':', color = '#595f69')\n",
    "        \n",
    "        plt.xlabel('Observations LTE', fontsize = 12)\n",
    "        plt.ylabel('Predictions LTE', fontsize = 12)\n",
    "        plt.legend()\n",
    "        # ~~~~~~~~~~~~~~~~~~CHANGED HERE~~~~~~~~~~~~~~~~~~~~\n",
    "        # Added fold number\n",
    "        plt.savefig(\"{}/Plot_predictions_fold{}.png\".format(output_dir, fold)', bbox_inches='tight', dpi=80)\n",
    "        plt.close()\n",
    "        \n",
    "        del x_train, x_valid\n",
    "        del y_train, y_valid\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Finished the \" + str(fold) + \" fold\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        fold += 1\n",
    "    else:   \n",
    "        print(\"Skipped the \" + str(fold) + \" fold\")\n",
    "        print(\"----------------------------------------------------\")  \n",
    "        fold += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(17):\n",
    "    print(f)\n",
    "    test_fold = pd.read_csv(\"./Test_predictions_fold{}_PCA20_2step_5min_rmseratio5_yields5.csv\".format(f), sep=\",\", header = None).values \n",
    "\n",
    "    # Getting rid of first zeros\n",
    "    #test_fold = test_fold[:,1:50]\n",
    "    \n",
    "    final_prediction_test[f*5:f*5+5] = np.mean(sc_y.inverse_transform(test_fold), axis = 1).reshape((5,1))\n",
    "    final_std_test[f*5:f*5+5] = np.std(sc_y.inverse_transform(test_fold), axis = 1).reshape((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(Y_data, final_prediction_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
